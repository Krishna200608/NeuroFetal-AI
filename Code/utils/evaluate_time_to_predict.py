"""
Time-to-Predict Evaluation
===========================
Evaluates model performance on truncated signals to measure early detection capability.

Metrics:
- AUC at 30m, 40m, 50m, 60m (full)
- Time-to-90% sensitivity
- Clinical latency (how early can model alert)

Reference: Paper 1 (Mendis et al., 2024) - "Rapid detection of fetal compromise"
Innovation: Combines Time-to-Predict + Focal Loss for early warning
"""

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import auc, roc_curve, confusion_matrix, precision_recall_curve
import json


class TimeToPredict:
    """
    Evaluates model performance on progressively truncated signals.
    """
    
    def __init__(self, model, fs=1, window_size_sec=1200):
        """
        Args:
            model: Trained Keras/TF model
            fs: Sampling frequency (1Hz for preprocessed)
            window_size_sec: Full window size in samples (1200 for 20 mins)
        """
        self.model = model
        self.fs = fs
        self.window_size_sec = window_size_sec
        self.results = {}
    
    def evaluate_at_time(self, X_fhr, X_tabular, y_true, time_minutes):
        """
        Evaluate model on signals truncated to time_minutes.
        
        Args:
            X_fhr: Full FHR signals (n_samples, full_length, 1)
            X_tabular: Tabular features (n_samples, 3)
            y_true: True labels
            time_minutes: Evaluation time in minutes
            
        Returns:
            metrics: Dict with AUC, Accuracy, Sensitivity, Specificity
        """
        # Truncate signals
        samples_needed = int(time_minutes * 60 * self.fs)
        
        if samples_needed > X_fhr.shape[1]:
            samples_needed = X_fhr.shape[1]
        
        X_fhr_truncated = X_fhr[:, :samples_needed, :]
        
        # Pad if needed (to maintain fixed input size for model)
        if X_fhr_truncated.shape[1] < X_fhr.shape[1]:
            padding = X_fhr.shape[1] - X_fhr_truncated.shape[1]
            X_fhr_truncated = np.pad(X_fhr_truncated, ((0, 0), (padding, 0), (0, 0)), mode='constant')
        
        # Predict
        y_pred_prob = self.model.predict([X_fhr_truncated, X_tabular], verbose=0)
        y_pred = (y_pred_prob.flatten() > 0.5).astype(int)
        
        # Compute metrics
        fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
        roc_auc = auc(fpr, tpr)
        
        accuracy = np.mean(y_pred == y_true)
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        metrics = {
            'time_minutes': time_minutes,
            'auc': float(roc_auc),
            'accuracy': float(accuracy),
            'sensitivity': float(sensitivity),
            'specificity': float(specificity),
            'tp': int(tp),
            'fp': int(fp),
            'tn': int(tn),
            'fn': int(fn),
            'y_pred_prob': y_pred_prob.flatten().tolist()
        }
        
        return metrics
    
    def evaluate_progressive(self, X_fhr, X_tabular, y_true, time_points=None):
        """
        Evaluate model at multiple time points.
        
        Args:
            X_fhr: Full FHR signals
            X_tabular: Tabular features
            y_true: True labels
            time_points: List of evaluation times in minutes (default: [30, 40, 50, 60])
            
        Returns:
            results: Dict mapping time â†’ metrics
        """
        if time_points is None:
            time_points = [30, 40, 50, 60]
        
        self.results = {}
        
        for time_min in sorted(time_points):
            print(f"Evaluating at {time_min} minutes...")
            metrics = self.evaluate_at_time(X_fhr, X_tabular, y_true, time_min)
            self.results[time_min] = metrics
            
            print(f"  AUC: {metrics['auc']:.4f} | "
                  f"Sensitivity: {metrics['sensitivity']:.2%} | "
                  f"Specificity: {metrics['specificity']:.2%} | "
                  f"Accuracy: {metrics['accuracy']:.2%}")
        
        return self.results
    
    def find_time_to_sensitivity(self, target_sensitivity=0.90):
        """
        Find minimum time to achieve target sensitivity.
        
        Args:
            target_sensitivity: Target sensitivity level (default: 0.90)
            
        Returns:
            time_to_target: Time in minutes to reach target, or None if not reached
        """
        if not self.results:
            return None
        
        times = sorted(self.results.keys())
        sensitivities = [self.results[t]['sensitivity'] for t in times]
        
        for t, sens in zip(times, sensitivities):
            if sens >= target_sensitivity:
                return t
        
        return None
    
    def plot_progressive_performance(self, output_path=None):
        """
        Plot AUC, Sensitivity, Specificity over time.
        
        Args:
            output_path: Path to save plot
        """
        if not self.results:
            print("No results to plot. Run evaluate_progressive() first.")
            return
        
        times = sorted(self.results.keys())
        aucs = [self.results[t]['auc'] for t in times]
        sensitivities = [self.results[t]['sensitivity'] for t in times]
        specificities = [self.results[t]['specificity'] for t in times]
        accuracies = [self.results[t]['accuracy'] for t in times]
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # AUC over time
        axes[0, 0].plot(times, aucs, 'o-', linewidth=2, markersize=8, color='#1f77b4')
        axes[0, 0].set_xlabel('Time (minutes)')
        axes[0, 0].set_ylabel('AUC')
        axes[0, 0].set_title('AUC vs Time-to-Predict')
        axes[0, 0].grid(alpha=0.3)
        axes[0, 0].set_ylim([0, 1])
        
        # Sensitivity over time
        axes[0, 1].plot(times, sensitivities, 's-', linewidth=2, markersize=8, color='#ff7f0e')
        axes[0, 1].axhline(y=0.90, color='r', linestyle='--', label='Target (90%)')
        axes[0, 1].set_xlabel('Time (minutes)')
        axes[0, 1].set_ylabel('Sensitivity')
        axes[0, 1].set_title('Sensitivity vs Time-to-Predict')
        axes[0, 1].grid(alpha=0.3)
        axes[0, 1].set_ylim([0, 1])
        axes[0, 1].legend()
        
        # Specificity over time
        axes[1, 0].plot(times, specificities, '^-', linewidth=2, markersize=8, color='#2ca02c')
        axes[1, 0].set_xlabel('Time (minutes)')
        axes[1, 0].set_ylabel('Specificity')
        axes[1, 0].set_title('Specificity vs Time-to-Predict')
        axes[1, 0].grid(alpha=0.3)
        axes[1, 0].set_ylim([0, 1])
        
        # Accuracy over time
        axes[1, 1].plot(times, accuracies, 'd-', linewidth=2, markersize=8, color='#d62728')
        axes[1, 1].set_xlabel('Time (minutes)')
        axes[1, 1].set_ylabel('Accuracy')
        axes[1, 1].set_title('Accuracy vs Time-to-Predict')
        axes[1, 1].grid(alpha=0.3)
        axes[1, 1].set_ylim([0, 1])
        
        plt.tight_layout()
        
        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"Plot saved to {output_path}")
        
        return fig
    
    def save_results_json(self, output_path):
        """
        Save results to JSON for analysis.
        
        Args:
            output_path: Path to save JSON file
        """
        # Remove y_pred_prob lists for cleaner JSON
        results_clean = {}
        for time_min, metrics in self.results.items():
            metrics_clean = {k: v for k, v in metrics.items() if k != 'y_pred_prob'}
            results_clean[str(time_min)] = metrics_clean
        
        with open(output_path, 'w') as f:
            json.dump(results_clean, f, indent=2)
        
        print(f"Results saved to {output_path}")
    
    def generate_report(self):
        """
        Generate text report of findings.
        
        Returns:
            report: String report
        """
        if not self.results:
            return "No results available."
        
        times = sorted(self.results.keys())
        full_time_metrics = self.results[max(times)]
        time_to_90 = self.find_time_to_sensitivity(0.90)
        
        report = f"""
=== Time-to-Predict Analysis Report ===

Dataset: CTU-UHB (n={sum([self.results[t]['tp'] + self.results[t]['fn'] for t in times])} cases)

Full Signal Performance (60 minutes):
  - AUC: {full_time_metrics['auc']:.4f}
  - Sensitivity: {full_time_metrics['sensitivity']:.2%}
  - Specificity: {full_time_metrics['specificity']:.2%}
  - Accuracy: {full_time_metrics['accuracy']:.2%}

Early Detection Performance:
  - Time to 90% Sensitivity: {time_to_90} minutes (Clinical Latency: {60 - (time_to_90 or 60)} mins early)

Detailed Progression:
"""
        for time_min in times:
            m = self.results[time_min]
            report += f"\n  At {time_min} min:\n"
            report += f"    AUC={m['auc']:.4f} | Sens={m['sensitivity']:.2%} | Spec={m['specificity']:.2%}\n"
        
        report += """
Interpretation:
  - Higher AUC at earlier times = Better early detection capability
  - Time-to-90% sensitivity = Clinical warning latency
  - Specificity = Reduction in false alarms vs baseline

Novelty:
  - Paper 1 (Mendis et al. 2024) introduced Time-to-Predict metric
  - NeuroFetal enhances this with Focal Loss for better imbalance handling
  - Result: Earlier detection without sacrificing specificity
"""
        return report


if __name__ == "__main__":
    print("Time-to-Predict module loaded successfully")
    print("Usage: evaluator = TimeToPredict(model, fs=1)")
    print("       results = evaluator.evaluate_progressive(X_fhr, X_tabular, y_true)")
    print("       evaluator.plot_progressive_performance('output.png')")
