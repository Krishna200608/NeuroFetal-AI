{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Baseline Model 1: 1D-CNN (Spilka et al., 2016)\n",
                "\n",
                "**Objective:** Reproduce the Deep Learning baseline using a standard 1D-CNN architecture on FHR signals ONLY (Unimodal).\n",
                "\n",
                "**Reference:** Spilka, J., et al. (2016). *Deep Learning for Fetal Heart Rate Analysis.*\n",
                "\n",
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# --- GitHub & Colab Setup ---\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    \n",
                "    # 1. Clone Repo using Secret Token\n",
                "    token = userdata.get('GITHUB_AUTH_TOKEN')\n",
                "    repo_name = \"NeuroFetal-AI\"\n",
                "    username = \"Krishna200608\"\n",
                "    repo_url = f\"https://{token}@github.com/{username}/{repo_name}.git\"\n",
                "    \n",
                "    if not os.path.exists(repo_name):\n",
                "        print(f\"Cloning {repo_name}...\")\n",
                "        get_ipython().system(f\"git clone {repo_url}\")\n",
                "    \n",
                "    # 2. Configure Git\n",
                "    os.chdir(repo_name)\n",
                "    get_ipython().system('git config --global user.email \"krishnasikheriya001@gmail.com\"')\n",
                "    get_ipython().system('git config --global user.name \"Krishna200608\"')\n",
                "    \n",
                "    # 3. Install Dependencies\n",
                "    get_ipython().system('pip install wfdb')\n",
                "\n",
                "    BASE_DIR = os.getcwd()\n",
                "    sys.path.append(os.path.join(BASE_DIR, \"Code\", \"scripts\"))\n",
                "    print(\"Running in Colab (GitHub Integration Active)\")\n",
                "\n",
                "except ImportError:\n",
                "    # Local Fallback\n",
                "    BASE_DIR = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
                "    sys.path.append(os.path.abspath(os.path.join(\"..\", \"scripts\")))\n",
                "    print(\"Running Locally\")\n",
                "\n",
                "import data_ingestion\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Process Data\n",
                "Using the existing `data_ingestion` pipeline. If data is missing locally, we attempt to run the ingestion script."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parameters\n",
                "WINDOW_SIZE = 2400  # 20 minutes at 2Hz\n",
                "STRIDE = 300\n",
                "\n",
                "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, \"Datasets\", \"processed\")\n",
                "X_path = os.path.join(PROCESSED_DATA_DIR, \"X_fhr.npy\")\n",
                "y_path = os.path.join(PROCESSED_DATA_DIR, \"y.npy\")\n",
                "\n",
                "# Check if data exists, if not, run ingestion\n",
                "if not os.path.exists(X_path) or not os.path.exists(y_path):\n",
                "    print(\"Processed data not found. Running data_ingestion.py...\")\n",
                "    # This assumes data_ingestion.py handles download/processing\n",
                "    get_ipython().system(f\"python Code/scripts/data_ingestion.py\")\n",
                "\n",
                "try:\n",
                "    X_signal = np.load(X_path)\n",
                "    y = np.load(y_path)\n",
                "    print(f\"Loaded Data: X_signal {X_signal.shape}, y {y.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: Data ingestion failed or data not found.\")\n",
                "    # Stop execution if critical\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define Baseline CNN (Spilka et al.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_baseline_cnn(input_shape=(2400, 1)):\n",
                "    inputs = keras.Input(shape=input_shape)\n",
                "    \n",
                "    # Block 1\n",
                "    x = layers.Conv1D(filters=16, kernel_size=7, strides=1, padding='same')(inputs)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.ReLU()(x)\n",
                "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
                "    \n",
                "    # Block 2\n",
                "    x = layers.Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.ReLU()(x)\n",
                "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
                "    \n",
                "    # Block 3\n",
                "    x = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='same')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.ReLU()(x)\n",
                "    x = layers.GlobalAveragePooling1D()(x)\n",
                "    \n",
                "    # Dense Classification Head\n",
                "    x = layers.Dense(64, activation='relu')(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
                "    \n",
                "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Baseline_CNN\")\n",
                "    return model\n",
                "\n",
                "model = build_baseline_cnn(input_shape=(X_signal.shape[1], 1))\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training (5-Fold CV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "aucs = []\n",
                "accs = []\n",
                "\n",
                "X = X_signal\n",
                "if X.ndim == 2:\n",
                "    X = np.expand_dims(X, axis=-1)\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
                "    print(f\"\\nStarting Fold {fold+1}...\")\n",
                "    \n",
                "    X_train, X_val = X[train_idx], X[val_idx]\n",
                "    y_train, y_val = y[train_idx], y[val_idx]\n",
                "    \n",
                "    # Class weights\n",
                "    y_train = y_train.astype(int)\n",
                "    neg, pos = np.bincount(y_train)\n",
                "    class_weight = {0: 1.0, 1: (neg / pos) if pos > 0 else 1.0}\n",
                "    \n",
                "    model = build_baseline_cnn(input_shape=(X.shape[1], 1))\n",
                "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC', 'accuracy'])\n",
                "    \n",
                "    early_stopping = keras.callbacks.EarlyStopping(\n",
                "        monitor='val_auc', patience=10, mode='max', restore_best_weights=True\n",
                "    )\n",
                "    \n",
                "    history = model.fit(\n",
                "        X_train, y_train,\n",
                "        validation_data=(X_val, y_val),\n",
                "        epochs=50,\n",
                "        batch_size=32,\n",
                "        class_weight=class_weight,\n",
                "        callbacks=[early_stopping],\n",
                "        verbose=1\n",
                "    )\n",
                "    \n",
                "    y_pred = model.predict(X_val, verbose=0)\n",
                "    auc = roc_auc_score(y_val, y_pred)\n",
                "    acc = accuracy_score(y_val, (y_pred > 0.5).astype(int))\n",
                "    \n",
                "    aucs.append(auc)\n",
                "    accs.append(acc)\n",
                "    print(f\"Fold {fold+1} Result -> AUC: {auc:.4f}, Acc: {acc:.4f}\")\n",
                "    \n",
                "    # Save Best Model\n",
                "    if len(aucs) == 1 or auc > max(aucs[:-1]):\n",
                "        model_dir = os.path.join(BASE_DIR, \"Code\", \"Baseline\", \"Models\")\n",
                "        os.makedirs(model_dir, exist_ok=True)\n",
                "        model_save_path = os.path.join(model_dir, \"baseline_paper3_best_cnn.keras\")\n",
                "        model.save(model_save_path)\n",
                "        print(f\"  Saved best model to {model_save_path}\")\n",
                "\n",
                "print(\"\\n=== Final Results ===\")\n",
                "mean_auc = np.mean(aucs)\n",
                "std_auc = np.std(aucs)\n",
                "mean_acc = np.mean(accs)\n",
                "print(f\"Mean AUC: {mean_auc:.4f} +/- {std_auc:.4f}\")\n",
                "print(f\"Mean Acc: {mean_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Save Results & Push to GitHub\n",
                "\n",
                "results_path = os.path.join(BASE_DIR, \"Code\", \"Baseline\", \"baseline_paper3_results.txt\")\n",
                "with open(results_path, \"w\") as f:\n",
                "    f.write(f\"Ref: Paper 3 (Spilka 2016) - CNN Baseline (Colab Run)\\n\")\n",
                "    f.write(f\"Mean AUC: {mean_auc:.4f}\\n\")\n",
                "    f.write(f\"Std Dev: {std_auc:.4f}\\n\")\n",
                "    f.write(f\"Mean Acc: {mean_acc:.4f}\\n\")\n",
                "print(f\"Results saved to {results_path}\")\n",
                "\n",
                "# Git Commit & Push\n",
                "try:\n",
                "    if 'google.colab' in sys.modules:\n",
                "        print(\"Pushing results to GitHub...\")\n",
                "        # Ensure we are in repo root\n",
                "        os.chdir(BASE_DIR)\n",
                "        \n",
                "        # Configure again just in case\n",
                "        get_ipython().system('git config --global user.email \"krishnasikheriya001@gmail.com\"')\n",
                "        get_ipython().system('git config --global user.name \"Krishna200608\"')\n",
                "        \n",
                "        # Pull latest changes to avoid conflicts\n",
                "        get_ipython().system('git pull origin main')\n",
                "        \n",
                "        # Add relevant files\n",
                "        get_ipython().system('git add Code/Baseline/Models/*.keras')\n",
                "        get_ipython().system('git add Code/Baseline/*.txt')\n",
                "        \n",
                "        # Commit\n",
                "        get_ipython().system('git commit -m \"Update CNN Baseline Results (Colab)\"')\n",
                "        \n",
                "        # Push\n",
                "        get_ipython().system('git push origin main')\n",
                "        print(\"Successfully pushed to GitHub!\")\n",
                "except Exception as e:\n",
                "    print(f\"Git Push Failed: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}