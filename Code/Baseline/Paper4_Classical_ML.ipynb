{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0aGqj3cznOV"
      },
      "source": [
        "# Baseline Model 2: Classical ML (Petrozziello et al., 2018)\n",
        "\n",
        "**Objective:** Reproduce Classical Machine Learning baselines (Logistic Regression, Random Forest) using handcrafted features.\n",
        "\n",
        "**Reference:** Petrozziello, A., et al. (2018). *Multimedia Information Retrieval for Multimodal Fetal Monitoring.*\n",
        "\n",
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59HLzeWQznOa",
        "outputId": "a4c966d2-14bc-4c44-aa35-8db9903bb4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Running in Colab (GitHub Integration Active)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "# --- GitHub & Colab Setup ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # 1. Clone Repo using Secret Token\n",
        "    token = userdata.get('GITHUB_AUTH_TOKEN')\n",
        "    repo_name = \"NeuroFetal-AI\"\n",
        "    username = \"Krishna200608\"\n",
        "    repo_url = f\"https://{token}@github.com/{username}/{repo_name}.git\"\n",
        "\n",
        "    if not os.path.exists(repo_name):\n",
        "        print(f\"Cloning {repo_name}...\")\n",
        "        get_ipython().system(f\"git clone {repo_url}\")\n",
        "\n",
        "    # 2. Configure Git\n",
        "    os.chdir(repo_name)\n",
        "    get_ipython().system('git config --global user.email \"krishnasikheriya001@gmail.com\"')\n",
        "    get_ipython().system('git config --global user.name \"Krishna200608\"')\n",
        "\n",
        "    # 3. Install Dependencies\n",
        "    get_ipython().system('pip install wfdb')\n",
        "\n",
        "    BASE_DIR = os.getcwd()\n",
        "    sys.path.append(os.path.join(BASE_DIR, \"Code\", \"scripts\"))\n",
        "    print(\"Running in Colab (GitHub Integration Active)\")\n",
        "\n",
        "except ImportError:\n",
        "    # Local Fallback\n",
        "    BASE_DIR = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
        "    sys.path.append(os.path.abspath(os.path.join(\"..\", \"scripts\")))\n",
        "    print(\"Running Locally\")\n",
        "\n",
        "import data_ingestion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhb9Tgt3znOe"
      },
      "source": [
        "## 2. Load Tabular Data\n",
        "Run `data_ingestion.py` if needed to generate `X_tabular.npy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rwmtM8aznOg",
        "outputId": "b9490ee5-b311-40e4-995b-83d5e2d75ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tabular features: (2546, 18)\n"
          ]
        }
      ],
      "source": [
        "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, \"Datasets\", \"processed\")\n",
        "X_path = os.path.join(PROCESSED_DATA_DIR, \"X_tabular.npy\")\n",
        "y_path = os.path.join(PROCESSED_DATA_DIR, \"y.npy\")\n",
        "\n",
        "# Check if data exists, if not, try ingestion\n",
        "if not os.path.exists(X_path) or not os.path.exists(y_path):\n",
        "    print(\"Tabular data not found. Running ingestion to extract features...\")\n",
        "    get_ipython().system(f\"python Code/scripts/data_ingestion.py\")\n",
        "\n",
        "try:\n",
        "    X_tabular = np.load(X_path)\n",
        "    y = np.load(y_path)\n",
        "    print(f\"Loaded tabular features: {X_tabular.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Features not found. Ensure ingestion ran successfully.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ft5XiAvznOh"
      },
      "source": [
        "## 3. Train & Evaluate Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LSHHQj8znOi",
        "outputId": "df5b82a8-9fd4-4a9d-d6d8-9896dfdbdd27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Cross-Validation...\n",
            "Fold 1\n",
            "Fold 2\n",
            "Fold 3\n",
            "Fold 4\n",
            "Fold 5\n",
            "\n",
            "=== Results ===\n",
            "Logistic Regression AUC: 0.6764 +/- 0.0166\n",
            "Logistic Regression Acc: 0.6473\n",
            "Random Forest AUC: 0.8373 +/- 0.0251\n",
            "Random Forest Acc: 0.8256\n",
            "Models saved to /content/NeuroFetal-AI/Code/Baseline/Models\n"
          ]
        }
      ],
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(class_weight='balanced', max_iter=2000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "}\n",
        "\n",
        "scores = {name: {'auc': [], 'acc': []} for name in models}\n",
        "X = np.nan_to_num(X_tabular, nan=0.0)\n",
        "\n",
        "print(\"Starting Cross-Validation...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        clf = model\n",
        "        if name == \"Random Forest\":\n",
        "            clf.fit(X_train, y_train)\n",
        "            input_val = X_val\n",
        "        else:\n",
        "            clf.fit(X_train_scaled, y_train)\n",
        "            input_val = X_val_scaled\n",
        "\n",
        "        if hasattr(clf, \"predict_proba\"):\n",
        "            y_pred_prob = clf.predict_proba(input_val)[:, 1]\n",
        "        else:\n",
        "            y_pred_prob = clf.predict(input_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, (y_pred_prob > 0.5).astype(int))\n",
        "        auc = roc_auc_score(y_val, y_pred_prob)\n",
        "\n",
        "        scores[name]['auc'].append(auc)\n",
        "        scores[name]['acc'].append(acc)\n",
        "\n",
        "        # Save Model (Update: Saving the last fold model for checking)\n",
        "        model_dir = os.path.join(BASE_DIR, \"Code\", \"Baseline\", \"Models\")\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        model_path = os.path.join(model_dir, f\"baseline_paper4_{name.replace(' ', '_')}.pkl\")\n",
        "        joblib.dump(clf, model_path)\n",
        "\n",
        "print(\"\\n=== Results ===\")\n",
        "for name in models:\n",
        "    print(f\"{name} AUC: {np.mean(scores[name]['auc']):.4f} +/- {np.std(scores[name]['auc']):.4f}\")\n",
        "    print(f\"{name} Acc: {np.mean(scores[name]['acc']):.4f}\")\n",
        "\n",
        "print(f\"Models saved to {model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAVfAbhLznOj",
        "outputId": "dc72c930-8dc3-44dd-8594-4b342e648bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to /content/NeuroFetal-AI/Code/Baseline/baseline_paper4_results.txt\n",
            "Pushing results to GitHub...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 5 (delta 3), reused 5 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (5/5), 13.92 KiB | 1.26 MiB/s, done.\n",
            "From https://github.com/Krishna200608/NeuroFetal-AI\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   ed00edf..0c1319b  main       -> origin/main\n",
            "Updating ed00edf..0c1319b\n",
            "Fast-forward\n",
            " Code/Baseline/Paper3_CNN.ipynb | 1389 \u001b[32m++++++++++++++++++++++++++++++++\u001b[m\u001b[31m--------\u001b[m\n",
            " 1 file changed, 1125 insertions(+), 264 deletions(-)\n",
            "[main e2ef64c] Update Classical ML Baseline Results (Colab)\n",
            " 3 files changed, 1 insertion(+), 1 deletion(-)\n",
            " create mode 100644 Code/Baseline/Models/baseline_paper4_Logistic_Regression.pkl\n",
            " create mode 100644 Code/Baseline/Models/baseline_paper4_Random_Forest.pkl\n",
            "Enumerating objects: 13, done.\n",
            "Counting objects: 100% (13/13), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (8/8), done.\n",
            "Writing objects: 100% (8/8), 1.10 MiB | 2.33 MiB/s, done.\n",
            "Total 8 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/Krishna200608/NeuroFetal-AI.git\n",
            "   0c1319b..e2ef64c  main -> main\n",
            "Successfully pushed to GitHub!\n"
          ]
        }
      ],
      "source": [
        "# 4. Save Results & Push to GitHub\n",
        "results_path = os.path.join(BASE_DIR, \"Code\", \"Baseline\", \"baseline_paper4_results.txt\")\n",
        "\n",
        "with open(results_path, \"w\") as f:\n",
        "    f.write(f\"Ref: Paper 4 (Petrozziello 2018) - Classical ML Baseline (Colab Run)\\n\")\n",
        "    for name in models:\n",
        "        mean_auc = np.mean(scores[name]['auc'])\n",
        "        std_auc = np.std(scores[name]['auc'])\n",
        "        mean_acc = np.mean(scores[name]['acc'])\n",
        "        f.write(f\"{name}: AUC {mean_auc:.4f} +/- {std_auc:.4f}, Acc {mean_acc:.4f}\\n\")\n",
        "print(f\"Results saved to {results_path}\")\n",
        "\n",
        "# Git Commit & Push\n",
        "try:\n",
        "    if 'google.colab' in sys.modules:\n",
        "        print(\"Pushing results to GitHub...\")\n",
        "        os.chdir(BASE_DIR)\n",
        "\n",
        "        get_ipython().system('git config --global user.email \"krishnasikheriya001@gmail.com\"')\n",
        "        get_ipython().system('git config --global user.name \"Krishna200608\"')\n",
        "\n",
        "        get_ipython().system('git pull origin main')\n",
        "        get_ipython().system('git add Code/Baseline/Models/*.pkl Code/Baseline/*.txt')\n",
        "        get_ipython().system('git commit -m \"Update Classical ML Baseline Results (Colab)\"')\n",
        "        get_ipython().system('git push origin main')\n",
        "        print(\"Successfully pushed to GitHub!\")\n",
        "except Exception as e:\n",
        "    print(f\"Git Push Failed: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}