{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "# NeuroFetal AI - Modular Training Pipeline\n",
        "\n",
        "**Version 3.2** - Automated Secrets & TFLite Auto-Push\n",
        "\n",
        "This notebook acts as a central launcher for the modular scripts in the codebase. It ensures consistency between local development and Cloud training.\n",
        "\n",
        "### Steps:\n",
        "1.  **Setup Environment**: Clone repo & install dependencies (Uses Colab Secrets).\n",
        "2.  **Data Ingestion**: Process raw PhysioNet data.\n",
        "3.  **Train**: Run the deep learning training pipeline.\n",
        "4.  **Evaluate**: Run ensemble and uncertainty metrics.\n",
        "5.  **Serve**: Launch the dashboard (optional).\n",
        "6.  **Deploy**: Convert to TFLite and push to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "github_auth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def5e225-8b3a-49d8-865c-cdc5d174c6d9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1. GitHub Authentication\n",
        "GITHUB_REPO = \"Krishna200608/NeuroFetal-AI\"\n",
        "\n",
        "try:\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    print(\"✓ GitHub Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Error loading GITHUB_TOKEN from Secrets. Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token (PAT): \")\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "os.environ['GITHUB_REPO'] = GITHUB_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637098eb-e506-40b6-b923-6d56503e9eff"
      },
      "outputs": [],
      "source": [
        "# 2. Clone Repository\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# CRITICAL FIX: Reset to /content before deleting the repo folder\n",
        "# This prevents 'shell-init: error retrieving current directory'\n",
        "try:\n",
        "    os.chdir(\"/content\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clean up any previous clone to avoid conflicts\n",
        "if os.path.exists(\"/content/NeuroFetal-AI\"):\n",
        "    shutil.rmtree(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\n",
        "\n",
        "# Set paths\n",
        "os.chdir(\"/content/NeuroFetal-AI\")\n",
        "print(\"Cloned successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc7df1a-98bc-49c3-956f-788d02a3618a"
      },
      "outputs": [],
      "source": [
        "# 3. Install Dependencies\n",
        "print(\"Installing libraries...\")\n",
        "!pip install -q wfdb shap scipy imbalanced-learn pyngrok filterpy scikit-learn matplotlib seaborn pandas numpy tensorflow streamlit plotly python-dotenv\n",
        "print(\"Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "## 2. Data Ingestion\n",
        "Processes raw `.dat`/`.hea` files into clean `.npy` arrays for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ingestion"
      },
      "outputs": [],
      "source": [
        "# Run the data ingestion script\n",
        "!python Code/scripts/data_ingestion.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_5_md"
      },
      "source": [
        "## 2.5. Self-Supervised Pretraining (New)\n",
        "Train the Masked Autoencoder (MAE) on the FHR data to learn robust features before supervision.\n",
        "This saves encoder weights to `Code/models/pretrained_fhr_encoder.weights.h5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pretrain"
      },
      "outputs": [],
      "source": [
        "# Run the SSL pretraining script\n",
        "!python Code/scripts/pretrain.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "## 3. Training\n",
        "Train the Tri-Modal Attention Fusion ResNet using 5-Fold Cross-Validation.\n",
        "This script automatically handles:\n",
        "*   Class Balancing (SMOTE)\n",
        "*   Feature Extraction (CSP)\n",
        "*   Model Checkpointing (saving best `.keras` files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run the main training script\n",
        "!python Code/scripts/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "## 4. Advanced Evaluation\n",
        "Generate metrics for:\n",
        "1.  **Ensemble Performance**: Rank Averaging across folds (AUC maximization).\n",
        "2.  **Uncertainty Quantification**: Monte Carlo Dropout confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation"
      },
      "outputs": [],
      "source": [
        "print(\"Running Ensemble Evaluation (Rank Averaging)...\")\n",
        "!python Code/scripts/evaluate_ensemble.py\n",
        "\n",
        "print(\"\\nRunning Uncertainty Quantification...\")\n",
        "!python Code/scripts/evaluate_uncertainty.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "## 5. Launch Dashboard (Optional)\n",
        "Run the Streamlit app directly from Colab using `ngrok`.\n",
        "**Note**: You need an `NGROK_AUTH_TOKEN` set in your Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit plotly python-dotenv pyngrok"
      ],
      "metadata": {
        "id": "sf8srrt3C6Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_dashboard"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    print(\"✓ Ngrok Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Error loading NGROK_AUTH_TOKEN from Secrets. Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    auth_token = getpass(\"Enter Ngrok Auth Token manually: \")\n",
        "\n",
        "if auth_token:\n",
        "    with open(\"Code/.env\", \"w\") as f:\n",
        "        f.write(f\"NGROK_AUTH_TOKEN={auth_token}\\n\")\n",
        "\n",
        "print(\"Launching Streamlit App...\")\n",
        "!python Code/run_app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_md"
      },
      "source": [
        "## 6. Convert to TFLite (Mobile Deployment)\n",
        "Convert the best trained model to TFLite format and **PUSH to GitHub** automatically.\n",
        "You don't need to download anything manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_tflite",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a2a83e-9109-4a3d-c57f-7e2f9643eb5d"
      },
      "outputs": [],
      "source": [
        "print(\"Converting model to TFLite...\")\n",
        "!python Code/scripts/convert_to_tflite.py\n",
        "\n",
        "print(\"\\nPushing TFLite models to GitHub...\")\n",
        "# Configure Git identity (required for commit)\n",
        "!git config --global user.email \"krishnasikheriya001@gmail.com\"\n",
        "!git config --global user.name \"Krishna200608\"\n",
        "\n",
        "# Add TFLite models\n",
        "!git add Code/models/tflite/*.tflite\n",
        "\n",
        "# Commit and Push\n",
        "# We use '|| true' to prevent error if nothing to commit (e.g. running twice)\n",
        "!git commit -m \"chore: Auto-update TFLite models from Colab\" || true\n",
        "!git push origin main\n",
        "\n",
        "print(\"✓ Models pushed to GitHub successfully! Check the repo.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}