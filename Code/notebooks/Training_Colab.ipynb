{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "# NeuroFetal AI - Modular Training Pipeline\n",
        "\n",
        "**Version 3.1** - TFLite Auto-Push\n",
        "\n",
        "This notebook acts as a central launcher for the modular scripts in the codebase. It ensures consistency between local development and Cloud training.\n",
        "\n",
        "### Steps:\n",
        "1.  **Setup Environment**: Clone repo & install dependencies.\n",
        "2.  **Data Ingestion**: Process raw PhysioNet data.\n",
        "3.  **Train**: Run the deep learning training pipeline.\n",
        "4.  **Evaluate**: Run ensemble and uncertainty metrics.\n",
        "5.  **Serve**: Launch the dashboard (optional).\n",
        "6.  **Deploy**: Convert to TFLite and push to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "github_auth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def5e225-8b3a-49d8-865c-cdc5d174c6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your GitHub Personal Access Token (PAT):\n",
            "··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# 1. GitHub Authentication\n",
        "GITHUB_REPO = \"Krishna200608/NeuroFetal-AI\"\n",
        "print(\"Please enter your GitHub Personal Access Token (PAT):\")\n",
        "GITHUB_TOKEN = getpass()\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "os.environ['GITHUB_REPO'] = GITHUB_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "clone_repo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637098eb-e506-40b6-b923-6d56503e9eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository...\n",
            "Cloning into 'NeuroFetal-AI'...\n",
            "remote: Enumerating objects: 1721, done.\u001b[K\n",
            "remote: Counting objects: 100% (205/205), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 1721 (delta 145), reused 108 (delta 61), pack-reused 1516 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1721/1721), 232.92 MiB | 22.41 MiB/s, done.\n",
            "Resolving deltas: 100% (920/920), done.\n",
            "Updating files: 100% (1182/1182), done.\n",
            "Cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# 2. Clone Repository\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# CRITICAL FIX: Reset to /content before deleting the repo folder\n",
        "# This prevents 'shell-init: error retrieving current directory'\n",
        "try:\n",
        "    os.chdir(\"/content\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clean up any previous clone to avoid conflicts\n",
        "if os.path.exists(\"/content/NeuroFetal-AI\"):\n",
        "    shutil.rmtree(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\n",
        "\n",
        "# Set paths\n",
        "os.chdir(\"/content/NeuroFetal-AI\")\n",
        "print(\"Cloned successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc7df1a-98bc-49c3-956f-788d02a3618a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed.\n"
          ]
        }
      ],
      "source": [
        "# 3. Install Dependencies\n",
        "print(\"Installing libraries...\")\n",
        "!pip install -q wfdb shap scipy imbalanced-learn pyngrok filterpy scikit-learn matplotlib seaborn pandas numpy tensorflow streamlit plotly python-dotenv\n",
        "print(\"Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "## 2. Data Ingestion\n",
        "Processes raw `.dat`/`.hea` files into clean `.npy` arrays for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ingestion"
      },
      "outputs": [],
      "source": [
        "# Run the data ingestion script\n",
        "!python Code/scripts/data_ingestion.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "## 3. Training\n",
        "Train the Tri-Modal Attention Fusion ResNet using 5-Fold Cross-Validation.\n",
        "This script automatically handles:\n",
        "*   Class Balancing (SMOTE)\n",
        "*   Feature Extraction (CSP)\n",
        "*   Model Checkpointing (saving best `.keras` files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run the main training script\n",
        "!python Code/scripts/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "## 4. Advanced Evaluation\n",
        "Generate metrics for:\n",
        "1.  **Ensemble Performance**: Rank Averaging across folds (AUC maximization).\n",
        "2.  **Uncertainty Quantification**: Monte Carlo Dropout confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation"
      },
      "outputs": [],
      "source": [
        "print(\"Running Ensemble Evaluation (Rank Averaging)...\")\n",
        "!python Code/scripts/evaluate_ensemble.py\n",
        "\n",
        "print(\"\\nRunning Uncertainty Quantification...\")\n",
        "!python Code/scripts/evaluate_uncertainty.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "## 5. Launch Dashboard (Optional)\n",
        "Run the Streamlit app directly from Colab using `ngrok`.\n",
        "**Note**: You need an `NGROK_AUTH_TOKEN` set in your `.env` or pasted below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit plotly python-dotenv pyngrok"
      ],
      "metadata": {
        "id": "sf8srrt3C6Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_dashboard"
      },
      "outputs": [],
      "source": [
        "# Create a .env file locally for certain secrets if needed\n",
        "auth_token = getpass(\"Enter Ngrok Auth Token (Validation optional if using .env): \")\n",
        "\n",
        "if auth_token:\n",
        "    with open(\"Code/.env\", \"w\") as f:\n",
        "        f.write(f\"NGROK_AUTH_TOKEN={auth_token}\\n\")\n",
        "\n",
        "print(\"Launching Streamlit App...\")\n",
        "!python Code/run_app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_md"
      },
      "source": [
        "## 6. Convert to TFLite (Mobile Deployment)\n",
        "Convert the best trained model to TFLite format and **PUSH to GitHub** automatically.\n",
        "You don't need to download anything manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_tflite",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a2a83e-9109-4a3d-c57f-7e2f9643eb5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting model to TFLite...\n",
            "2026-02-05 18:24:32.018791: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-02-05 18:24:32.025352: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-02-05 18:24:32.045918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770315872.081656    1399 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770315872.092100    1399 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770315872.117349    1399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770315872.117400    1399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770315872.117408    1399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770315872.117417    1399 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-05 18:24:32.125420: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Warning: Direct import failed: cannot import name 'TemporalAttention' from 'attention_blocks' (/content/NeuroFetal-AI/Code/utils/attention_blocks.py)\n",
            "Warning: Package import failed: cannot import name 'TemporalAttention' from 'utils.attention_blocks' (/content/NeuroFetal-AI/Code/utils/attention_blocks.py)\n",
            "Defining robust dummy classes for loading.\n",
            "Loading Keras model from: /content/NeuroFetal-AI/Code/models/enhanced_model_fold_1.keras\n",
            "2026-02-05 18:24:38.814284: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model loaded successfully.\n",
            "Converting to TFLite...\n"
          ]
        }
      ],
      "source": [
        "print(\"Converting model to TFLite...\")\n",
        "!python Code/scripts/convert_to_tflite.py\n",
        "\n",
        "print(\"\\nPushing TFLite models to GitHub...\")\n",
        "# Configure Git identity (required for commit)\n",
        "!git config --global user.email \"krishnasikheriya001@gmail.com\"\n",
        "!git config --global user.name \"Krishna200608\"\n",
        "\n",
        "# Add TFLite models\n",
        "!git add Code/models/tflite/*.tflite\n",
        "\n",
        "# Commit and Push\n",
        "# We use '|| true' to prevent error if nothing to commit (e.g. running twice)\n",
        "!git commit -m \"chore: Auto-update TFLite models from Colab\" || true\n",
        "!git push origin main\n",
        "\n",
        "print(\"✓ Models pushed to GitHub successfully! Check the repo.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}