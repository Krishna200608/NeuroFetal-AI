{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "# NeuroFetal AI - Modular Training Pipeline\n",
        "\n",
        "**Version 3.2** - Automated Secrets & TFLite Auto-Push\n",
        "\n",
        "This notebook acts as a central launcher for the modular scripts in the codebase. It ensures consistency between local development and Cloud training.\n",
        "\n",
        "### Steps:\n",
        "1.  **Setup Environment**: Clone repo & install dependencies (Uses Colab Secrets).\n",
        "2.  **Data Ingestion**: Process raw PhysioNet data.\n",
        "3.  **Train**: Run the deep learning training pipeline.\n",
        "4.  **Evaluate**: Run ensemble and uncertainty metrics.\n",
        "5.  **Serve**: Launch the dashboard (optional).\n",
        "6.  **Deploy**: Convert to TFLite and push to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "github_auth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a05f725-673e-4657-e16c-bdf20b385c3f"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1. GitHub Authentication\n",
        "GITHUB_REPO = \"Krishna200608/NeuroFetal-AI\"\n",
        "\n",
        "try:\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    print(\"\u2713 GitHub Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"\u26a0\ufe0f Error loading GITHUB_TOKEN from Secrets. Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token (PAT): \")\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "os.environ['GITHUB_REPO'] = GITHUB_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe6b7e4-94e0-4811-e435-a214c5cbea32"
      },
      "outputs": [],
      "source": [
        "# 2. Clone Repository\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# CRITICAL FIX: Reset to /content before deleting the repo folder\n",
        "# This prevents 'shell-init: error retrieving current directory'\n",
        "try:\n",
        "    os.chdir(\"/content\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clean up any previous clone to avoid conflicts\n",
        "if os.path.exists(\"/content/NeuroFetal-AI\"):\n",
        "    shutil.rmtree(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\n",
        "\n",
        "# Set paths\n",
        "os.chdir(\"/content/NeuroFetal-AI\")\n",
        "print(\"Cloned successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ea2ab7-bc0b-4086-d278-8f43986dcc70"
      },
      "outputs": [],
      "source": [
        "# 3. Install Dependencies\n",
        "print(\"Installing libraries...\")\n",
        "!pip install -q wfdb shap scipy imbalanced-learn pyngrok filterpy scikit-learn matplotlib seaborn pandas numpy tensorflow streamlit plotly python-dotenv\n",
        "print(\"Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "## 2. Data Ingestion\n",
        "Processes raw `.dat`/`.hea` files into clean `.npy` arrays for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ingestion",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f313146-a3b3-4324-e871-1b90d2b7fbff"
      },
      "outputs": [],
      "source": [
        "# Run the data ingestion script\n",
        "!python Code/scripts/data_ingestion.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_5_md"
      },
      "source": [
        "## 2.5. Self-Supervised Pretraining (New)\n",
        "Train the Masked Autoencoder (MAE) on the FHR data to learn robust features before supervision.\n",
        "This saves encoder weights to `Code/models/pretrained_fhr_encoder.weights.keras`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIL-g9Yd6oLd",
        "outputId": "d955123b-3247-4a2f-8aad-9a21a3a26481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pretrain",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fb1ece-d9bd-4387-bef7-f9fa9a6ef8aa"
      },
      "outputs": [],
      "source": [
        "# Run the SSL pretraining script\n",
        "!python Code/scripts/pretrain.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "## 3. Training\n",
        "Train the Tri-Modal Attention Fusion ResNet using 5-Fold Cross-Validation.\n",
        "This script automatically handles:\n",
        "*   Class Balancing (SMOTE)\n",
        "*   Feature Extraction (CSP)\n",
        "*   Model Checkpointing (saving best `.keras` files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7def5628-8758-4ede-c8fc-847b81609525"
      },
      "outputs": [],
      "source": [
        "# Training Loop with Git Push\n",
        "import os\n",
        "\n",
        "folds = 5\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "\n",
        "for fold in range(1, folds + 1):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"Training Fold {fold}/{folds}\")\n",
        "    print(f\"{'='*40}\")\n",
        "    \n",
        "    # Run training for this fold\n",
        "    !python Code/scripts/train.py --fold {fold} --epochs {epochs} --batch_size {batch_size}\n",
        "    \n",
        "    # Push to GitHub\n",
        "    model_path = f\"Code/models/enhanced_model_fold_{fold}.keras\"\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Pushing model for Fold {fold} to GitHub...\")\n",
        "        !git add {model_path}\n",
        "        !git commit -m \"Auto-save: Trained model for Fold {fold}\"\n",
        "        !git push origin main\n",
        "        print(f\"\u2713 Model for Fold {fold} pushed successfully.\")\n",
        "    else:\n",
        "        print(f\"\u26a0\ufe0f Model file not found: {model_path}\")\n",
        "\n",
        "# Evaluation after training\n",
        "print(\"\\nRunning Ensemble Evaluation (Rank Averaging)...\")\n",
        "!python Code/scripts/evaluate_ensemble.py\n",
        "\n",
        "print(\"\\nRunning Uncertainty Quantification...\")\n",
        "!python Code/scripts/evaluate_uncertainty.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "## 5. Launch Dashboard (Optional)\n",
        "Run the Streamlit app directly from Colab using `ngrok`.\n",
        "**Note**: You need an `NGROK_AUTH_TOKEN` set in your Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit plotly python-dotenv pyngrok"
      ],
      "metadata": {
        "id": "sf8srrt3C6Gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c18255-f9b9-496b-f05f-9072f2c4e43c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_dashboard",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7919094f-de08-4296-d9d9-643272772e9d"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    print(\"\u2713 Ngrok Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"\u26a0\ufe0f Error loading NGROK_AUTH_TOKEN from Secrets. Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    auth_token = getpass(\"Enter Ngrok Auth Token manually: \")\n",
        "\n",
        "if auth_token:\n",
        "    with open(\"Code/.env\", \"w\") as f:\n",
        "        f.write(f\"NGROK_AUTH_TOKEN={auth_token}\\n\")\n",
        "\n",
        "print(\"Launching Streamlit App...\")\n",
        "!python Code/run_app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_md"
      },
      "source": [
        "## 6. Convert to TFLite (Mobile Deployment)\n",
        "Convert the best trained model to TFLite format and **PUSH to GitHub** automatically.\n",
        "You don't need to download anything manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_tflite"
      },
      "outputs": [],
      "source": [
        "# Convert and Push TFLite Model\n",
        "!python Code/scripts/convert_to_tflite.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}