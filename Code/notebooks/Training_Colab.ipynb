{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "# NeuroFetal AI - Modular Training Pipeline\n",
        "\n",
        "**Version 3.0** - Simplified & Modular\n",
        "\n",
        "This notebook acts as a central launcher for the modular scripts in the codebase. It ensures consistency between local development and Cloud training.\n",
        "\n",
        "### Steps:\n",
        "1.  **Setup Environment**: Clone repo & install dependencies.\n",
        "2.  **Data Ingestion**: Process raw PhysioNet data.\n",
        "3.  **Train**: Run the deep learning training pipeline.\n",
        "4.  **Evaluate**: Run ensemble and uncertainty metrics.\n",
        "5.  **Serve**: Launch the dashboard (optional)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "github_auth"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# 1. GitHub Authentication\n",
        "GITHUB_REPO = \"Krishna200608/NeuroFetal-AI\"\n",
        "print(\"Please enter your GitHub Personal Access Token (PAT):\")\n",
        "GITHUB_TOKEN = getpass()\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "os.environ['GITHUB_REPO'] = GITHUB_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# 2. Clone Repository\n",
        "import shutil\n",
        "\n",
        "# Clean up any previous clone to avoid conflicts\n",
        "if os.path.exists(\"/content/NeuroFetal-AI\"):\n",
        "    shutil.rmtree(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\n",
        "\n",
        "# Set paths\n",
        "os.chdir(\"/content/NeuroFetal-AI\")\n",
        "print(\"Cloned successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# 3. Install Dependencies\n",
        "print(\"Installing libraries...\")\n",
        "!pip install -q wfdb shap scipy imbalanced-learn pyngrok filterpy scikit-learn matplotlib seaborn pandas numpy tensorflow streamlit plotly python-dotenv\n",
        "print(\"Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "## 2. Data Ingestion\n",
        "Processes raw `.dat`/`.hea` files into clean `.npy` arrays for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ingestion"
      },
      "outputs": [],
      "source": [
        "# Run the data ingestion script\n",
        "!python Code/scripts/data_ingestion.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "## 3. Training\n",
        "Train the Tri-Modal Attention Fusion ResNet using 5-Fold Cross-Validation.\n",
        "This script automatically handles:\n",
        "*   Class Balancing (SMOTE)\n",
        "*   Feature Extraction (CSP)\n",
        "*   Model Checkpointing (saving best `.keras` files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run the main training script\n",
        "!python Code/scripts/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "## 4. Advanced Evaluation\n",
        "Generate metrics for:\n",
        "1.  **Ensemble Performance**: Rank Averaging across folds (AUC maximization).\n",
        "2.  **Uncertainty Quantification**: Monte Carlo Dropout confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation"
      },
      "outputs": [],
      "source": [
        "print(\"Running Ensemble Evaluation (Rank Averaging)...\")\n",
        "!python Code/scripts/evaluate_ensemble.py\n",
        "\n",
        "print(\"\\nRunning Uncertainty Quantification...\")\n",
        "!python Code/scripts/evaluate_uncertainty.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "## 5. Launch Dashboard (Optional)\n",
        "Run the Streamlit app directly from Colab using `ngrok`.\n",
        "**Note**: You need an `NGROK_AUTH_TOKEN` set in your `.env` or pasted below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_dashboard"
      },
      "outputs": [],
      "source": [
        "# Create a .env file locally for certain secrets if needed\n",
        "auth_token = getpass(\"Enter Ngrok Auth Token (Validation optional if using .env): \")\n",
        "\n",
        "if auth_token:\n",
        "    with open(\"Code/.env\", \"w\") as f:\n",
        "        f.write(f\"NGROK_AUTH_TOKEN={auth_token}\\n\")\n",
        "\n",
        "print(\"Launching Streamlit App...\")\n",
        "!python Code/run_app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}