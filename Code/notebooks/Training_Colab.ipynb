{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "# NeuroFetal AI — SOTA Training Pipeline\n",
        "\n",
        "**Version 4.0** — 6-Phase SOTA Strategy (AUC 0.84+ Target)\n",
        "\n",
        "This notebook orchestrates the full SOTA pipeline on Google Colab with GPU acceleration.\n",
        "\n",
        "### Pipeline Steps\n",
        "| # | Phase | Script | Expected AUC Lift |\n",
        "|---|-------|--------|-------------------|\n",
        "| 1 | Setup | Clone repo, install deps | — |\n",
        "| 2 | Data Ingestion | `data_ingestion.py` — 18 features, pH 7.15, quality filter | +5–8 pts |\n",
        "| 3 | SSL Pretraining | `pretrain.py` — Masked Autoencoder on FHR | +2–3 pts |\n",
        "| 4 | Primary Training | `train.py` — 200 epochs, focal loss, 4x augment | +3–5 pts |\n",
        "| 5 | Ensemble Training | `train_diverse_ensemble.py` — InceptionNet + XGB + Stacking | +3–5 pts |\n",
        "| 6 | Evaluation | `evaluate_ensemble.py` — Temp scaling, TTA, calibration | +1–2 pts |\n",
        "| 7 | Deployment | `convert_to_tflite.py` — TFLite & auto-push | — |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "github_auth"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1. GitHub Authentication\n",
        "GITHUB_REPO = \"Krishna200608/NeuroFetal-AI\"\n",
        "\n",
        "try:\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    print(\"✓ GitHub Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Error loading GITHUB_TOKEN from Secrets. Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token (PAT): \")\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "os.environ['GITHUB_REPO'] = GITHUB_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# 2. Clone Repository\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Reset to /content before deleting the repo folder\n",
        "try:\n",
        "    os.chdir(\"/content\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clean up any previous clone\n",
        "if os.path.exists(\"/content/NeuroFetal-AI\"):\n",
        "    shutil.rmtree(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\n",
        "\n",
        "os.chdir(\"/content/NeuroFetal-AI\")\n",
        "print(\"✓ Cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "git_config_md"
      },
      "source": [
        "### 1.5 Git Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "git_config"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"krishnasikheriya001@gmail.com\"\n",
        "!git config --global user.name \"Krishna200608\"\n",
        "print(\"✓ Git credentials set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deps_md"
      },
      "source": [
        "### 1.6 Install Dependencies\n",
        "Installs all packages required for the full SOTA pipeline (including XGBoost/LightGBM for ensemble)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "print(\"Installing libraries...\")\n",
        "!pip install -q wfdb shap scipy imbalanced-learn pyngrok filterpy \\\n",
        "    scikit-learn matplotlib seaborn pandas numpy tensorflow \\\n",
        "    streamlit plotly python-dotenv xgboost lightgbm\n",
        "print(\"✓ Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "---\n",
        "## 2. Data Ingestion (Phase 1–2)\n",
        "\n",
        "Processes raw `.dat`/`.hea` files into clean `.npy` arrays.\n",
        "\n",
        "**SOTA enhancements:**\n",
        "- 18 tabular features (13 signal-derived: STV, LTV, accels/decels, baseline, variability…)\n",
        "- FHR normalization excluding 0-gaps\n",
        "- pH threshold relaxed to 7.15 (FIGO)\n",
        "- Signal quality filter (skip >50% loss)\n",
        "- Feature standardization (Z-score) with saved scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ingestion"
      },
      "outputs": [],
      "source": [
        "!python Code/scripts/data_ingestion.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_5_md"
      },
      "source": [
        "---\n",
        "## 3. Self-Supervised Pretraining\n",
        "\n",
        "Train the Masked Autoencoder (MAE) on unlabelled FHR data to learn robust temporal representations.\n",
        "\n",
        "Saves encoder weights → `Code/models/pretrained_fhr_encoder.weights.keras`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pull_before_pretrain"
      },
      "outputs": [],
      "source": [
        "!git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pretrain"
      },
      "outputs": [],
      "source": [
        "!python Code/scripts/pretrain.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "---\n",
        "## 4. Primary Model Training (Phase 3–4)\n",
        "\n",
        "Train the **AttentionFusionResNet** using 5-Fold Cross-Validation.\n",
        "\n",
        "**SOTA enhancements:**\n",
        "- 200 epochs with cosine annealing + warmup\n",
        "- Focal Loss (α=0.65, γ=2.0) — less aggressive for better calibration\n",
        "- 4x data augmentation (SpecAugment + CutMix + time-warp + jitter + mixup)\n",
        "- AdamW with weight decay 5e-4\n",
        "- Backbone right-sized to 192-dim with stochastic depth\n",
        "- Auxiliary pH regression head for multi-task learning\n",
        "- Early stopping patience = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pull_before_train"
      },
      "outputs": [],
      "source": [
        "!git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Full 5-fold training — approx 2-3 hours on T4 GPU\n",
        "!python Code/scripts/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "push_models"
      },
      "outputs": [],
      "source": [
        "# Auto-push trained models to GitHub\n",
        "import os\n",
        "\n",
        "for fold in range(1, 6):\n",
        "    model_path = f\"Code/models/enhanced_model_fold_{fold}.keras\"\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Pushing model for Fold {fold}...\")\n",
        "        !git add {model_path}\n",
        "        !git commit -m \"Auto-save: Trained SOTA model Fold {fold}\"\n",
        "        !git push origin main\n",
        "        print(f\"✓ Fold {fold} pushed.\")\n",
        "    else:\n",
        "        print(f\"⚠️ Not found: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "---\n",
        "## 5. Diverse Ensemble Training (Phase 5)\n",
        "\n",
        "Train three diverse model families and combine with a stacking meta-learner:\n",
        "\n",
        "1. **AttentionFusionResNet** — primary (already trained above)\n",
        "2. **1D-InceptionNet** — multi-scale temporal patterns (kernel 5/15/40)\n",
        "3. **XGBoost / LightGBM** — gradient boosting on tabular + CSP + FHR features\n",
        "\n",
        "Out-of-fold predictions across 5 folds → Logistic Regression stacking\n",
        "\n",
        "**Expected additional AUC lift: +3–5 pts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ensemble_training"
      },
      "outputs": [],
      "source": [
        "!python Code/scripts/train_diverse_ensemble.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "push_ensemble"
      },
      "outputs": [],
      "source": [
        "# Push ensemble artifacts\n",
        "import os\n",
        "\n",
        "ensemble_files = [\n",
        "    \"Code/models/stacking_meta_learner.pkl\",\n",
        "    \"Code/models/xgb_model.pkl\",\n",
        "]\n",
        "\n",
        "# Also push any InceptionNet fold models\n",
        "for fold in range(1, 6):\n",
        "    inception_path = f\"Code/models/inception_fold_{fold}.keras\"\n",
        "    if os.path.exists(inception_path):\n",
        "        ensemble_files.append(inception_path)\n",
        "\n",
        "pushed = []\n",
        "for f in ensemble_files:\n",
        "    if os.path.exists(f):\n",
        "        !git add {f}\n",
        "        pushed.append(f)\n",
        "\n",
        "if pushed:\n",
        "    !git commit -m \"Auto-save: Diverse ensemble models (InceptionNet + XGB + meta-learner)\"\n",
        "    !git push origin main\n",
        "    print(f\"✓ Pushed {len(pushed)} ensemble artifacts.\")\n",
        "else:\n",
        "    print(\"⚠️ No ensemble files found to push.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_5_md"
      },
      "source": [
        "---\n",
        "## 6. Evaluation & Calibration (Phase 6)\n",
        "\n",
        "**Stacking Ensemble Evaluation** with:\n",
        "- Temperature scaling (Guo et al., 2017)\n",
        "- Optimal threshold search (Youden's J / F1 / cost-sensitive)\n",
        "- Enhanced 3-pass TTA (original + flip + noise)\n",
        "- AUPRC reporting for imbalanced data\n",
        "\n",
        "**Uncertainty Quantification** via MC Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_eval"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRunning Stacking Ensemble Evaluation...\")\n",
        "!python Code/scripts/evaluate_ensemble.py\n",
        "\n",
        "print(\"\\nRunning Uncertainty Quantification (MC Dropout)...\")\n",
        "!python Code/scripts/evaluate_uncertainty.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "---\n",
        "## 7. Launch Dashboard (Optional)\n",
        "\n",
        "Run the Streamlit dashboard from Colab via **ngrok** tunnel.\n",
        "\n",
        "> Requires `NGROK_AUTH_TOKEN` in Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_dashboard"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    print(\"✓ Ngrok Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Error loading NGROK_AUTH_TOKEN from Secrets. Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    auth_token = getpass(\"Enter Ngrok Auth Token manually: \")\n",
        "\n",
        "if auth_token:\n",
        "    with open(\"Code/.env\", \"w\") as f:\n",
        "        f.write(f\"NGROK_AUTH_TOKEN={auth_token}\\n\")\n",
        "\n",
        "print(\"Launching Streamlit App...\")\n",
        "!python Code/run_app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_md"
      },
      "source": [
        "---\n",
        "## 8. Convert to TFLite & Auto-Push\n",
        "\n",
        "Convert the best trained model to TFLite format and push to GitHub automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_tflite"
      },
      "outputs": [],
      "source": [
        "!python Code/scripts/convert_to_tflite.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "push_tflite"
      },
      "outputs": [],
      "source": [
        "# Push TFLite model\n",
        "import os\n",
        "\n",
        "tflite_path = \"Code/models/model.tflite\"\n",
        "if os.path.exists(tflite_path):\n",
        "    !git add {tflite_path}\n",
        "    !git commit -m \"Auto-save: TFLite model\"\n",
        "    !git push origin main\n",
        "    print(\"✓ TFLite model pushed.\")\n",
        "else:\n",
        "    print(\"⚠️ TFLite model not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "done_md"
      },
      "source": [
        "---\n",
        "## ✅ Pipeline Complete\n",
        "\n",
        "All 6 SOTA phases have been executed. Check the evaluation output above for final AUC and calibration metrics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}