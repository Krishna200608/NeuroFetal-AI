{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_md"
      },
      "source": [
        "# NeuroFetal AI — V4.0 TimeGAN Synthetic CTG Generation\n",
        "\n",
        "**Branch:** `feat/v4.0-timegan`\n",
        "\n",
        "**Objective:** Train a 1D Convolutional GAN on the minority-class (Pathological) FHR+UC signals to generate realistic synthetic fetal distress traces, replacing SMOTE for class balancing.\n",
        "\n",
        "### Pipeline\n",
        "| # | Phase | Description |\n",
        "|---|-------|-------------|\n",
        "| 1 | Setup | Clone repo, install deps, authenticate |\n",
        "| 2 | Data Prep | Load `.npy`, isolate pathological traces, stack FHR+UC |\n",
        "| 3 | Architecture | Build 1D Conv Generator + Discriminator |\n",
        "| 4 | Training | Custom `tf.GradientTape` loop with WGAN-GP |\n",
        "| 5 | Visualization | Compare real vs synthetic traces |\n",
        "| 6 | Export | Save synthetic `.npy` files |\n",
        "| 7 | Push | Commit results back to GitHub |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "---\n",
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "github_auth"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GITHUB_REPO = \"Krishna200608/NeuroFetal-AI\"\n",
        "\n",
        "try:\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    print(\"✓ GitHub Token loaded from Secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Falling back to manual input.\")\n",
        "    from getpass import getpass\n",
        "    GITHUB_TOKEN = getpass(\"Enter GitHub PAT: \")\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "os.environ['GITHUB_REPO'] = GITHUB_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "import shutil, os\n",
        "\n",
        "try:\n",
        "    os.chdir(\"/content\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if os.path.exists(\"/content/NeuroFetal-AI\"):\n",
        "    shutil.rmtree(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\n",
        "os.chdir(\"/content/NeuroFetal-AI\")\n",
        "\n",
        "!git config --global user.email \"krishnasikheriya001@gmail.com\"\n",
        "!git config --global user.name \"Krishna200608\"\n",
        "\n",
        "!git fetch origin\n",
        "!git checkout feat/v4.0-timegan\n",
        "print(\"✓ Cloned and checked out feat/v4.0-timegan!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "!pip install -q wfdb scipy imbalanced-learn scikit-learn matplotlib seaborn pandas numpy tensorflow\n",
        "print(\"✓ Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "---\n",
        "## 2. Data Ingestion & Pathological Isolation\n",
        "\n",
        "Run the existing data pipeline, then isolate **only** the minority class (`y == 1`, Compromised pH < 7.15). We stack FHR + UC into a `(N, 1200, 2)` tensor to preserve their physiological cross-correlation (late decelerations following contractions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_ingestion"
      },
      "outputs": [],
      "source": [
        "# Run standard data ingestion first\n",
        "!python Code/scripts/data_ingestion.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_and_filter"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ── Load preprocessed data ──\n",
        "X_fhr = np.load(\"Datasets/processed/X_fhr.npy\")\n",
        "X_uc  = np.load(\"Datasets/processed/X_uc.npy\")\n",
        "X_tab = np.load(\"Datasets/processed/X_tabular.npy\")\n",
        "y     = np.load(\"Datasets/processed/y.npy\")\n",
        "\n",
        "print(f\"Full dataset — FHR: {X_fhr.shape}, UC: {X_uc.shape}, Tab: {X_tab.shape}, y: {y.shape}\")\n",
        "print(f\"Class balance: {y.sum():.0f} pathological / {len(y)} total ({y.mean()*100:.2f}%)\")\n",
        "\n",
        "# ── Isolate minority class ──\n",
        "patho_idx = np.where(y == 1)[0]\n",
        "X_fhr_patho = X_fhr[patho_idx]  # (N_patho, 1200, 1)\n",
        "X_uc_patho  = X_uc[patho_idx]   # (N_patho, 1200, 1)\n",
        "\n",
        "# ── Stack FHR + UC into 2-channel signal ──\n",
        "# Shape: (N_patho, 1200, 2) — Channel 0 = FHR, Channel 1 = UC\n",
        "X_patho_stacked = np.concatenate([X_fhr_patho, X_uc_patho], axis=-1)\n",
        "\n",
        "print(f\"\\nPathological samples isolated: {X_patho_stacked.shape[0]}\")\n",
        "print(f\"Stacked signal shape: {X_patho_stacked.shape}  (timesteps=1200, channels=2)\")\n",
        "\n",
        "# ── Quick visualization of a real pathological trace ──\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 5), sharex=True)\n",
        "sample_idx = 0\n",
        "axes[0].plot(X_patho_stacked[sample_idx, :, 0], color='crimson', linewidth=0.8)\n",
        "axes[0].set_ylabel('FHR (normalized)')\n",
        "axes[0].set_title(f'Real Pathological Trace #{sample_idx}')\n",
        "axes[1].plot(X_patho_stacked[sample_idx, :, 1], color='steelblue', linewidth=0.8)\n",
        "axes[1].set_ylabel('UC (normalized)')\n",
        "axes[1].set_xlabel('Timestep (1 Hz, 20 min window)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "---\n",
        "## 3. TimeGAN Architecture (1D Convolutional WGAN-GP)\n",
        "\n",
        "We use a **Wasserstein GAN with Gradient Penalty (WGAN-GP)** architecture built entirely with 1D Convolutions.\n",
        "\n",
        "### Design Rationale\n",
        "- **Why not RNNs?** At sequence length 1200, LSTMs/GRUs suffer from vanishing gradients and are extremely slow to train.\n",
        "- **Why WGAN-GP?** Standard GAN training with BCE loss is notoriously unstable. WGAN-GP uses the Wasserstein distance + gradient penalty for much more stable convergence on small datasets.\n",
        "- **Why 2-channel?** By generating FHR and UC simultaneously, we preserve their physiological cross-correlation (e.g., late decelerations follow contractions).\n",
        "\n",
        "### Architecture Summary\n",
        "| Component | Input | Output | Key Layers |\n",
        "|-----------|-------|--------|------------|\n",
        "| **Generator** | `(batch, 128)` noise | `(batch, 1200, 2)` | Dense → Reshape → 5x Conv1DTranspose (upsampling) → tanh |\n",
        "| **Critic** | `(batch, 1200, 2)` signal | `(batch, 1)` score | 5x Conv1D (downsampling) → Dense → linear |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build_gan"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# Hyperparameters\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "NOISE_DIM      = 128       # Latent space dimension\n",
        "SEQ_LEN        = 1200      # 20 min at 1 Hz\n",
        "N_CHANNELS     = 2         # FHR + UC\n",
        "BATCH_SIZE     = 16        # Small for Colab T4 VRAM\n",
        "GP_WEIGHT      = 10.0      # Gradient penalty coefficient\n",
        "N_CRITIC       = 5         # Critic updates per generator update\n",
        "EPOCHS         = 500       # Total training epochs\n",
        "LR_G           = 1e-4      # Generator learning rate\n",
        "LR_D           = 1e-4      # Critic learning rate\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# GENERATOR: Noise → (1200, 2) synthetic CTG signal\n",
        "# Strategy: Dense → Reshape to (75, 256) → 4 Conv1DTranspose blocks\n",
        "# to progressively upsample: 75 → 150 → 300 → 600 → 1200\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "def build_generator():\n",
        "    model = keras.Sequential(name=\"Generator\")\n",
        "\n",
        "    # Project noise to a small temporal tensor\n",
        "    model.add(layers.Dense(75 * 256, input_dim=NOISE_DIM))\n",
        "    model.add(layers.Reshape((75, 256)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    # Block 1: 75 → 150\n",
        "    model.add(layers.Conv1DTranspose(256, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    # Block 2: 150 → 300\n",
        "    model.add(layers.Conv1DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    # Block 3: 300 → 600\n",
        "    model.add(layers.Conv1DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    # Block 4: 600 → 1200\n",
        "    model.add(layers.Conv1DTranspose(32, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    # Output: (1200, 2) — tanh for normalized signals\n",
        "    model.add(layers.Conv1D(N_CHANNELS, kernel_size=7, padding='same', activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# CRITIC (Discriminator): (1200, 2) signal → scalar Wasserstein score\n",
        "# Strategy: 4 Conv1D blocks to downsample: 1200 → 600 → 300 → 150 → 75\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "def build_critic():\n",
        "    model = keras.Sequential(name=\"Critic\")\n",
        "\n",
        "    # Block 1: 1200 → 600\n",
        "    model.add(layers.Conv1D(32, kernel_size=5, strides=2, padding='same', input_shape=(SEQ_LEN, N_CHANNELS)))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Block 2: 600 → 300\n",
        "    model.add(layers.Conv1D(64, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Block 3: 300 → 150\n",
        "    model.add(layers.Conv1D(128, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Block 4: 150 → 75\n",
        "    model.add(layers.Conv1D(256, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    # Flatten → score (no sigmoid for WGAN)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))  # Linear output for Wasserstein distance\n",
        "\n",
        "    return model\n",
        "\n",
        "# ── Build and inspect ──\n",
        "generator = build_generator()\n",
        "critic    = build_critic()\n",
        "\n",
        "print(\"=\"*60)\n",
        "generator.summary()\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "critic.summary()\n",
        "\n",
        "# Quick sanity check\n",
        "test_noise = tf.random.normal((1, NOISE_DIM))\n",
        "test_output = generator(test_noise)\n",
        "print(f\"\\n✓ Generator output shape: {test_output.shape}  (expected: (1, 1200, 2))\")\n",
        "test_score = critic(test_output)\n",
        "print(f\"✓ Critic output shape: {test_score.shape}  (expected: (1, 1))\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "---\n",
        "## 4. WGAN-GP Training Loop\n",
        "\n",
        "Custom training with:\n",
        "- **Wasserstein loss** (Critic maximizes `E[D(real)] - E[D(fake)]`)\n",
        "- **Gradient Penalty** for Lipschitz constraint enforcement\n",
        "- **5:1 Critic-to-Generator update ratio** for stable convergence\n",
        "- **Visualization** every 50 epochs to track morphological quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# ── Optimizers ──\n",
        "gen_optimizer    = keras.optimizers.Adam(LR_G, beta_1=0.0, beta_2=0.9)\n",
        "critic_optimizer = keras.optimizers.Adam(LR_D, beta_1=0.0, beta_2=0.9)\n",
        "\n",
        "# ── Prepare dataset ──\n",
        "# Normalize data to [-1, 1] range for tanh generator output\n",
        "data_min = X_patho_stacked.min(axis=(0, 1), keepdims=True)\n",
        "data_max = X_patho_stacked.max(axis=(0, 1), keepdims=True)\n",
        "data_range = data_max - data_min\n",
        "data_range[data_range == 0] = 1.0  # Avoid division by zero\n",
        "\n",
        "X_normalized = 2.0 * (X_patho_stacked - data_min) / data_range - 1.0\n",
        "X_normalized = X_normalized.astype(np.float32)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_normalized)\n",
        "dataset = dataset.shuffle(buffer_size=len(X_normalized)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(f\"Training data: {X_normalized.shape} normalized to [-1, 1]\")\n",
        "print(f\"Batches per epoch: {len(X_normalized) // BATCH_SIZE}\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# Gradient Penalty\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "@tf.function\n",
        "def gradient_penalty(real_samples, fake_samples):\n",
        "    \"\"\"Computes gradient penalty for WGAN-GP.\"\"\"\n",
        "    batch_size = tf.shape(real_samples)[0]\n",
        "    alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
        "    interpolated = real_samples + alpha * (fake_samples - real_samples)\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(interpolated)\n",
        "        pred = critic(interpolated, training=True)\n",
        "\n",
        "    grads = gp_tape.gradient(pred, interpolated)\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]) + 1e-8)\n",
        "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "    return gp\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# Single Training Step\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "@tf.function\n",
        "def train_critic_step(real_batch):\n",
        "    \"\"\"One critic update step.\"\"\"\n",
        "    noise = tf.random.normal((tf.shape(real_batch)[0], NOISE_DIM))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake_batch = generator(noise, training=True)\n",
        "        real_score  = critic(real_batch, training=True)\n",
        "        fake_score  = critic(fake_batch, training=True)\n",
        "\n",
        "        # Wasserstein loss: maximize E[D(real)] - E[D(fake)]\n",
        "        # Critic minimizes: E[D(fake)] - E[D(real)] + GP\n",
        "        w_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)\n",
        "        gp = gradient_penalty(real_batch, fake_batch)\n",
        "        critic_loss = w_loss + GP_WEIGHT * gp\n",
        "\n",
        "    grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
        "    critic_optimizer.apply_gradients(zip(grads, critic.trainable_variables))\n",
        "    return critic_loss, w_loss\n",
        "\n",
        "@tf.function\n",
        "def train_generator_step():\n",
        "    \"\"\"One generator update step.\"\"\"\n",
        "    noise = tf.random.normal((BATCH_SIZE, NOISE_DIM))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake_batch = generator(noise, training=True)\n",
        "        fake_score = critic(fake_batch, training=True)\n",
        "        # Generator wants critic to score fakes highly\n",
        "        gen_loss = -tf.reduce_mean(fake_score)\n",
        "\n",
        "    grads = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gen_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
        "    return gen_loss\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# Visualization Helper\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "def visualize_comparison(epoch, real_data, n_samples=3):\n",
        "    \"\"\"Plot real vs generated traces side by side.\"\"\"\n",
        "    noise = tf.random.normal((n_samples, NOISE_DIM))\n",
        "    generated = generator(noise, training=False).numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(n_samples, 2, figsize=(16, 3 * n_samples))\n",
        "    fig.suptitle(f'Epoch {epoch}: Real (Left) vs Generated (Right)', fontsize=14, fontweight='bold')\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Pick a random real sample\n",
        "        real_idx = np.random.randint(0, len(real_data))\n",
        "\n",
        "        # Real trace\n",
        "        axes[i, 0].plot(real_data[real_idx, :, 0], color='crimson', linewidth=0.7, label='FHR')\n",
        "        axes[i, 0].plot(real_data[real_idx, :, 1], color='steelblue', linewidth=0.7, alpha=0.7, label='UC')\n",
        "        axes[i, 0].set_ylabel(f'Sample {i+1}')\n",
        "        if i == 0:\n",
        "            axes[i, 0].set_title('REAL Pathological')\n",
        "            axes[i, 0].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "        # Generated trace\n",
        "        axes[i, 1].plot(generated[i, :, 0], color='crimson', linewidth=0.7, label='FHR')\n",
        "        axes[i, 1].plot(generated[i, :, 1], color='steelblue', linewidth=0.7, alpha=0.7, label='UC')\n",
        "        if i == 0:\n",
        "            axes[i, 1].set_title('GENERATED Synthetic')\n",
        "            axes[i, 1].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "    axes[-1, 0].set_xlabel('Timestep (1 Hz)')\n",
        "    axes[-1, 1].set_xlabel('Timestep (1 Hz)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'Code/models/gan_comparison_epoch_{epoch}.png', dpi=100, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# Main Training Loop\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting WGAN-GP Training — {EPOCHS} epochs\")\n",
        "print(f\"Generator LR: {LR_G}, Critic LR: {LR_D}\")\n",
        "print(f\"Critic updates per G update: {N_CRITIC}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "history = {'d_loss': [], 'g_loss': [], 'w_dist': []}\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_d_loss = []\n",
        "    epoch_g_loss = []\n",
        "    epoch_w_dist = []\n",
        "\n",
        "    for step, real_batch in enumerate(dataset):\n",
        "        # ── Train Critic N_CRITIC times ──\n",
        "        d_loss, w_dist = train_critic_step(real_batch)\n",
        "        epoch_d_loss.append(float(d_loss))\n",
        "        epoch_w_dist.append(float(w_dist))\n",
        "\n",
        "        # ── Train Generator once every N_CRITIC steps ──\n",
        "        if step % N_CRITIC == 0:\n",
        "            g_loss = train_generator_step()\n",
        "            epoch_g_loss.append(float(g_loss))\n",
        "\n",
        "    # Record epoch averages\n",
        "    avg_d = np.mean(epoch_d_loss)\n",
        "    avg_g = np.mean(epoch_g_loss) if epoch_g_loss else 0\n",
        "    avg_w = np.mean(epoch_w_dist)\n",
        "    history['d_loss'].append(avg_d)\n",
        "    history['g_loss'].append(avg_g)\n",
        "    history['w_dist'].append(avg_w)\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Epoch {epoch:>4d}/{EPOCHS} | D Loss: {avg_d:>8.4f} | G Loss: {avg_g:>8.4f} | W Dist: {avg_w:>8.4f} | Time: {elapsed:.0f}s\")\n",
        "\n",
        "    # Visualize every 50 epochs\n",
        "    if epoch % 50 == 0:\n",
        "        visualize_comparison(epoch, X_normalized, n_samples=3)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n✓ Training complete in {total_time/60:.1f} minutes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "---\n",
        "## 5. Training Diagnostics\n",
        "\n",
        "Plot the loss curves to verify stable convergence. For WGAN-GP:\n",
        "- The **Wasserstein distance** (W Dist) should decrease over time, indicating the generator is fooling the critic.\n",
        "- Loss curves should **not** diverge wildly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_losses"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "axes[0].plot(history['d_loss'], color='orange', linewidth=0.8)\n",
        "axes[0].set_title('Critic Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(history['g_loss'], color='green', linewidth=0.8)\n",
        "axes[1].set_title('Generator Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "axes[2].plot(history['w_dist'], color='purple', linewidth=0.8)\n",
        "axes[2].set_title('Wasserstein Distance')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('WGAN-GP Training Diagnostics', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Code/models/gan_training_diagnostics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_md"
      },
      "source": [
        "---\n",
        "## 6. Generate & Export Synthetic Data\n",
        "\n",
        "Generate **3x** the original pathological count as new synthetic samples. These will be used to replace SMOTE in the V4.0 training pipeline.\n",
        "\n",
        "We also perform a statistical sanity check: compare mean/std of real vs synthetic per channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_synthetic"
      },
      "outputs": [],
      "source": [
        "# ── Configuration ──\n",
        "N_SYNTHETIC = len(X_fhr_patho) * 3  # 3x oversampling\n",
        "print(f\"Generating {N_SYNTHETIC} synthetic pathological traces...\")\n",
        "\n",
        "# ── Batch generation to avoid OOM ──\n",
        "synthetic_batches = []\n",
        "gen_batch_size = 64\n",
        "for i in range(0, N_SYNTHETIC, gen_batch_size):\n",
        "    batch_n = min(gen_batch_size, N_SYNTHETIC - i)\n",
        "    noise = tf.random.normal((batch_n, NOISE_DIM))\n",
        "    synthetic_batch = generator(noise, training=False).numpy()\n",
        "    synthetic_batches.append(synthetic_batch)\n",
        "\n",
        "X_synthetic_normalized = np.concatenate(synthetic_batches, axis=0)  # (N_SYNTHETIC, 1200, 2)\n",
        "\n",
        "# ── De-normalize back to original scale ──\n",
        "X_synthetic = (X_synthetic_normalized + 1.0) / 2.0 * data_range + data_min\n",
        "\n",
        "# ── Split back into FHR and UC ──\n",
        "X_fhr_synthetic = X_synthetic[:, :, 0:1]  # (N, 1200, 1)\n",
        "X_uc_synthetic  = X_synthetic[:, :, 1:2]   # (N, 1200, 1)\n",
        "\n",
        "print(f\"\\nSynthetic FHR shape: {X_fhr_synthetic.shape}\")\n",
        "print(f\"Synthetic UC  shape: {X_uc_synthetic.shape}\")\n",
        "\n",
        "# ── Statistical Validation ──\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Statistical Comparison: Real vs Synthetic\")\n",
        "print(f\"{'='*50}\")\n",
        "for ch, name in enumerate(['FHR', 'UC']):\n",
        "    real_mean = X_patho_stacked[:, :, ch].mean()\n",
        "    real_std  = X_patho_stacked[:, :, ch].std()\n",
        "    syn_mean  = X_synthetic[:, :, ch].mean()\n",
        "    syn_std   = X_synthetic[:, :, ch].std()\n",
        "    print(f\"  {name} — Real: mean={real_mean:.4f} std={real_std:.4f} | Synth: mean={syn_mean:.4f} std={syn_std:.4f}\")\n",
        "\n",
        "# ── Save synthetic arrays ──\n",
        "os.makedirs(\"Datasets/synthetic\", exist_ok=True)\n",
        "np.save(\"Datasets/synthetic/X_fhr_synthetic.npy\", X_fhr_synthetic)\n",
        "np.save(\"Datasets/synthetic/X_uc_synthetic.npy\", X_uc_synthetic)\n",
        "\n",
        "# ── Save generator weights ──\n",
        "generator.save(\"Code/models/generator_v4.keras\")\n",
        "\n",
        "# ── Save normalization params for reproducibility ──\n",
        "np.save(\"Datasets/synthetic/data_min.npy\", data_min)\n",
        "np.save(\"Datasets/synthetic/data_max.npy\", data_max)\n",
        "\n",
        "print(f\"\\n✓ Saved synthetic data to Datasets/synthetic/\")\n",
        "print(f\"✓ Saved generator weights to Code/models/generator_v4.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7_md"
      },
      "source": [
        "---\n",
        "## 7. Final Visual Comparison (Publication Quality)\n",
        "\n",
        "Generate a clean figure comparing 5 real and 5 synthetic traces — suitable for your mid-semester presentation or paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_viz"
      },
      "outputs": [],
      "source": [
        "n_show = 5\n",
        "fig, axes = plt.subplots(n_show, 2, figsize=(16, 2.5 * n_show))\n",
        "fig.suptitle('TimeGAN Output: Real vs Synthetic Pathological CTG Traces',\n",
        "             fontsize=15, fontweight='bold', y=1.01)\n",
        "\n",
        "for i in range(n_show):\n",
        "    real_idx = np.random.randint(0, len(X_fhr_patho))\n",
        "    syn_idx  = np.random.randint(0, len(X_fhr_synthetic))\n",
        "\n",
        "    # Real\n",
        "    axes[i, 0].plot(X_fhr_patho[real_idx, :, 0], color='crimson', linewidth=0.6, label='FHR')\n",
        "    ax2 = axes[i, 0].twinx()\n",
        "    ax2.plot(X_uc_patho[real_idx, :, 0], color='steelblue', linewidth=0.6, alpha=0.6, label='UC')\n",
        "    ax2.set_ylabel('UC', color='steelblue', fontsize=8)\n",
        "    axes[i, 0].set_ylabel('FHR', color='crimson', fontsize=8)\n",
        "\n",
        "    # Synthetic\n",
        "    axes[i, 1].plot(X_fhr_synthetic[syn_idx, :, 0], color='crimson', linewidth=0.6, label='FHR')\n",
        "    ax2s = axes[i, 1].twinx()\n",
        "    ax2s.plot(X_uc_synthetic[syn_idx, :, 0], color='steelblue', linewidth=0.6, alpha=0.6, label='UC')\n",
        "    ax2s.set_ylabel('UC', color='steelblue', fontsize=8)\n",
        "    axes[i, 1].set_ylabel('FHR', color='crimson', fontsize=8)\n",
        "\n",
        "axes[0, 0].set_title('REAL Pathological', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('SYNTHETIC (TimeGAN)', fontsize=12, fontweight='bold')\n",
        "axes[-1, 0].set_xlabel('Timestep (1 Hz, 20 min)')\n",
        "axes[-1, 1].set_xlabel('Timestep (1 Hz, 20 min)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Code/models/timegan_final_comparison.png', dpi=200, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Publication-quality comparison saved to Code/models/timegan_final_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8_md"
      },
      "source": [
        "---\n",
        "## 8. Integration Stub: V4.0 Ensemble Training\n",
        "\n",
        "The following code shows exactly how to inject the synthetic data into the existing training pipeline. This replaces SMOTE in the K-Fold loop inside `train_diverse_ensemble.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "integration_stub"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════\n",
        "# Integration Example: Replace SMOTE with TimeGAN Synthetic Data\n",
        "# Paste this logic into the K-Fold loop of train_diverse_ensemble.py\n",
        "# ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "def load_and_augment_with_timegan(X_fhr_train, X_uc_train, X_tab_train, y_train):\n",
        "    \"\"\"\n",
        "    Replaces SMOTE by injecting TimeGAN-generated synthetic pathological\n",
        "    traces into the training fold.\n",
        "\n",
        "    Returns augmented arrays with balanced classes.\n",
        "    \"\"\"\n",
        "    # Load synthetic data\n",
        "    X_fhr_syn = np.load(\"Datasets/synthetic/X_fhr_synthetic.npy\")\n",
        "    X_uc_syn  = np.load(\"Datasets/synthetic/X_uc_synthetic.npy\")\n",
        "\n",
        "    # Calculate how many synthetic samples we need to balance\n",
        "    n_positive = int(y_train.sum())\n",
        "    n_negative = len(y_train) - n_positive\n",
        "    n_needed   = n_negative - n_positive  # Number to add to balance\n",
        "\n",
        "    # Sample from synthetic pool (with replacement if needed)\n",
        "    syn_indices = np.random.choice(len(X_fhr_syn), size=min(n_needed, len(X_fhr_syn)), replace=False)\n",
        "\n",
        "    # Create synthetic tabular features by sampling from existing pathological\n",
        "    patho_tab_idx = np.where(y_train == 1)[0]\n",
        "    syn_tab_indices = np.random.choice(patho_tab_idx, size=len(syn_indices), replace=True)\n",
        "    X_tab_syn = X_tab_train[syn_tab_indices]\n",
        "\n",
        "    # Concatenate\n",
        "    X_fhr_aug = np.concatenate([X_fhr_train, X_fhr_syn[syn_indices]], axis=0)\n",
        "    X_uc_aug  = np.concatenate([X_uc_train, X_uc_syn[syn_indices]], axis=0)\n",
        "    X_tab_aug = np.concatenate([X_tab_train, X_tab_syn], axis=0)\n",
        "    y_aug     = np.concatenate([y_train, np.ones(len(syn_indices))], axis=0)\n",
        "\n",
        "    # Shuffle\n",
        "    perm = np.random.permutation(len(y_aug))\n",
        "    X_fhr_aug = X_fhr_aug[perm]\n",
        "    X_uc_aug  = X_uc_aug[perm]\n",
        "    X_tab_aug = X_tab_aug[perm]\n",
        "    y_aug     = y_aug[perm]\n",
        "\n",
        "    print(f\"  TimeGAN Aug: {n_positive} → {int(y_aug.sum())} positives / {len(y_aug)} total ({y_aug.mean()*100:.1f}%)\")\n",
        "    return X_fhr_aug, X_uc_aug, X_tab_aug, y_aug\n",
        "\n",
        "print(\"✓ Integration function defined. Ready for V4.0 ensemble training.\")\n",
        "print(\"  Usage: Replace the SMOTE block in train.py/train_diverse_ensemble.py\")\n",
        "print(\"  with a call to load_and_augment_with_timegan().\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9_md"
      },
      "source": [
        "---\n",
        "## 9. Push Results to GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "git_push"
      },
      "outputs": [],
      "source": [
        "!git add Datasets/synthetic/ Code/models/generator_v4.keras Code/models/gan_*.png Code/models/timegan_*.png\n",
        "!git commit -m \"feat(v4.0): TimeGAN synthetic CTG generation — {N_SYNTHETIC} pathological traces\"\n",
        "!git push origin feat/v4.0-timegan\n",
        "print(\"\\n✓ All results pushed to feat/v4.0-timegan branch!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}